{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage,HumanMessage,SystemMessage\n",
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate,load_prompt,MessagesPlaceholder\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate,FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.globals import set_llm_cache,set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory,ConversationSummaryBufferMemory,ConversationSummaryMemory,ConversationKGMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자주 쓰는 변수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CHAT.predict(\"좀 있다가 에슐리 퀸즈를 가는데, 먹을거 추천 좀 해줘봐\")\n",
    "# 예제에서 특징은 chat이라는 변수에 어떠한 모델을 사용하든, predict라는 메소드 사용 가능 / 답변은 매번 바뀜\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    SystemMessage(\n",
    "        content=\"넌 이제부터 매우 awsome하고 coooool한 음식의 specialist야. 그 어떠한 음식에 대해 물어보면 재료와 레시피에 대해 말할 수 있어.\"),\n",
    "    AIMessage(\n",
    "        content=\"안녕하세요. 저는 맛있는 녀석입니다.\"),\n",
    "    HumanMessage(\n",
    "        content=\"일식 덮밥집에서 파는 부타동에 대해 설명해줘.\")\n",
    "]\n",
    "\n",
    "CHAT.predict_messages(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom_template = PromptTemplate.from_template(\"{place}을(를) 가는데, 추천 좀 해줘봐\")\n",
    "#PromptTemplate는 단순 메세지를 프롬프트로 사용 할 때 유용함\n",
    "\n",
    "prom_template = prom_template.format(place = \"맘스터치\")\n",
    "\n",
    "CHAT.predict(prom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"넌 이제부터 매우 awsome하고 coooool한 {job}의 specialist야. 그 어떠한 질문이든 대답 할 수 있어.\"),\n",
    "    (\"ai\", \"안녕하세요. 저는 {name}입니다.\"),\n",
    "    (\"human\", \"{place}을(를) 가는데, 추천 좀 해줘.\"),\n",
    "])\n",
    "#ChatPromptTemplate는 조금 더 정교하게 설정된 프롬프트를 탬플릿을 통해 사용 할 때 유용함\n",
    "\n",
    "\n",
    "chat_template = chat_template.format(job=\"음식\",name=\"맛있는 녀석\",place=\"맘스터치\")\n",
    "\n",
    "CHAT.predict(chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BaseOutputParser를 사용하여 결과 출력을 parse하기\n",
    "\n",
    "class CommaOutParser(BaseOutputParser):\n",
    "    def parse(self,text): # 필수 메소드\n",
    "        items = text.strip().split(\".\")\n",
    "        return(list(map(str.strip,items)))\n",
    "    \n",
    "parser = CommaOutParser()\n",
    "parser.parse(\"  안녕하세요     .   유령개입니다 .    잘부탁드리겠습니다.\")\n",
    "\n",
    "#3분 24초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 질문이 들어오면, 최대 {n}개만큼의 리스트로 대답하는 리스트 대답 기계입니다. 리스트가 아닌걸론 대답하지 마세요. 그리고, 각 리스트들은 ,(콤마)로 구분되야해요.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chat_template = chat_template.format(\n",
    "    n = 10,\n",
    "    question = \"아시아에 위치한 나라를 알려주세요.  \"\n",
    ")\n",
    "\n",
    "rs = CHAT.predict(chat_template)\n",
    "parser.parse(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 두개와 달리, 더 간단하게 할 수 있는 버전이 있다. 여기선 오직 CHAT이라는 chatmodel 변수와, template. parser 세 개만이 존재한다.\n",
    "\n",
    "class CommaOutParser(BaseOutputParser):\n",
    "    def parse(self,text): # 필수 메소드\n",
    "        items = text.strip().split(\".\")\n",
    "        return(list(map(str.strip,items)))\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 질문이 들어오면, 최대 {n}개만큼의 리스트로 대답하는 리스트 대답 기계입니다. 리스트가 아닌걸론 대답하지 마세요. 그리고, 각 리스트들은 ,(콤마)로 구분되야해요.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "chain = chat_template | CHAT | CommaOutParser()\n",
    "chain.invoke({\n",
    "    \"n\":3,\n",
    "    \"question\":\"유럽의 나라\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 세계적인 슈퍼 클래스의 요리사입니다. 어떠한 요리를 요구하든 그 요리의 재료와 레시피에 대해 알려줄 수 있습니다.\"),\n",
    "    (\"human\", \"저는 {food} 요리를 만들고 싶습니다. 들어가는 재료와 레시피에 대해 알려주실 수 있을까요?\"),\n",
    "])\n",
    "veg_chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 어떠한 요리의 레시피가 들어와도 채식으로 변형시킬 수 있는 세계 대표의 채식 요리사입니다. 레시피가 들어오면, 해당 메뉴의 재료를 채식에 맞게 변형시켜야합니다.\"),\n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | CHAT\n",
    "veg_chef_chain = veg_chef_template | CHAT\n",
    "fianal_chain = {\"recipe\":chef_chain} | veg_chef_chain # chef_template | CHAT | veg_chef_template | CHAT << 이것과 같은 형태임.\n",
    "# 여러개의 체인을 위와 같은 방식으로 엮을 수 있음. 이건 머신러닝과 엇비슷한 형태라고 볼 수 있음\n",
    "\n",
    "fianal_chain.invoke({\n",
    "    \"food\": \"프랑스\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fewShort는 모델에게 예제를 준다. > 더 나은 대답을 받기 위해 / 프롬프트의 system에서 ai에거 답변 요구사항을 올리는것보단, 예제로 출력 형식을 잡아주는게 더 효율적임\n",
    "\n",
    "# 샘플들 > 이제 ai가 이러한 형태로 답변을 하게 됨\n",
    "\n",
    "few_short_example = [\n",
    "    {\n",
    "        \"question\":\"터키의 수도에 대해 알려줘.\",\n",
    "        \"answer\":\"\"\"\n",
    "        터키의 수도는 '앙카라'입니다. 면적은 면적은 약 2516㎢이며, 인구는 2010년 기준 4,431,719명입니다.\n",
    "        '앙카라'라는 지명은 '닻'을 의미하는 그리스어 앙퀴라에서 유래된 것으로 추측됩니다.\n",
    "        '앙카라'에는 앙카라 성채, 아나톨리아 문명박물관, 악칼레, 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"프랑스의 수도에 대해 알려줘.\",\n",
    "        \"answer\":\"\"\"\n",
    "        프랑스의 수도는 '파리'입니다. 면적은 면적은 105.4㎢이며, 인구는 2023년 기준 13,024,518명입니다.\n",
    "        '파리'라는 지명은 고대에 이 지역에 살았던 갈리아의 일족 파리시(Parisii)에서 유래되었습니다.\n",
    "        '파리'에는 에펠탑, 루브루 박물관 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"한국의 수도에 대해 알려줘.\",\n",
    "        \"answer\":\"\"\"\n",
    "        한국의 수도는 '서울'입니다. 면적은 면적은 약 605.2㎢이며, 인구는 2024년 기준 9,360,400명입니다.\n",
    "        '서울'라는 지명은 한국어의 순우리말로서 '한 나라의 수도(首都)', 곧 '국도(國都)'를 가리키는 일반명사입니다.\n",
    "        '서울'에는 광하문, 국립중앙박물관, 경복궁, 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate는 PromptTemplate 기반으로 예제를 사용함\n",
    "\n",
    "example_template =\"\"\"\n",
    "    Human:{question}\n",
    "    AI:{answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=few_short_example,\n",
    "    suffix=\"Human: {country}의 수도에 대해 알려줘.\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "chain = prompt | CHAT\n",
    "chain.invoke({\"country\":\"아시리아\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_short_chat_example = [\n",
    "    {\n",
    "        \"country\":\"터키\",\n",
    "        \"answer\":\"\"\"\n",
    "        터키의 수도는 '앙카라'입니다. 면적은 면적은 약 2516㎢이며, 인구는 2010년 기준 4,431,719명입니다.\n",
    "        '앙카라'라는 지명은 '닻'을 의미하는 그리스어 앙퀴라에서 유래된 것으로 추측됩니다.\n",
    "        '앙카라'에는 앙카라 성채, 아나톨리아 문명박물관, 악칼레, 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"country\":\"프랑스\",\n",
    "        \"answer\":\"\"\"\n",
    "        프랑스의 수도는 '파리'입니다. 면적은 면적은 105.4㎢이며, 인구는 2023년 기준 13,024,518명입니다.\n",
    "        '파리'라는 지명은 고대에 이 지역에 살았던 갈리아의 일족 파리시(Parisii)에서 유래되었습니다.\n",
    "        '파리'에는 에펠탑, 루브루 박물관 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"country\":\"한국\",\n",
    "        \"answer\":\"\"\"\n",
    "        한국의 수도는 '서울'입니다. 면적은 면적은 약 605.2㎢이며, 인구는 2024년 기준 9,360,400명입니다.\n",
    "        '서울'라는 지명은 한국어의 순우리말로서 '한 나라의 수도(首都)', 곧 '국도(國都)'를 가리키는 일반명사입니다.\n",
    "        '서울'에는 광하문, 국립중앙박물관, 경복궁, 등과 같은 관광지가 존재합니다.\n",
    "        \"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotChatMessagePromptTemplate은 ChatPromptTemplate을 기반으로하여 예제를 사용함\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"user\",\"{country}의 수도에 대해 알려줘.\"),\n",
    "        (\"ai\",\"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=few_short_chat_example,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"당신은 세계적인 지리 및 역사학자입니다. 특정 국가에 대해 물어보면 그게 얼마나 오래된 국가여도 대답해줄 수 있습니다.\"),\n",
    "    example_prompt,\n",
    "    (\"human\",\"{country}의 수도에 대해 알려줘.\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | CHAT\n",
    "chain.invoke({\"country\": \"몽골\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시를 랜덤으로 선택해주는 ExampleSelector. BaseExampleSelector를 상속받음.\n",
    "class ExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self,examples) -> None:\n",
    "        self.examples = examples    # 시작할때, 샘플을 받음.\n",
    "\n",
    "    def select_examples(self, input_variables): #파라미터로 받은 샘플을 고름\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=few_short_example,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "base_example_selector = ExampleSelector(\n",
    "    examples=few_short_example\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Human: {country}의 수도에 대해 알려줘.\",\n",
    "    input_variables=[\"country\"],\n",
    "    example_selector=base_example_selector\n",
    ")\n",
    "\n",
    "prompt.format(country=\"미국\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 에러 발생해서 아래와 같은 코드로 구성\n",
    "\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# JSON 파일을 UTF-8로 읽기\n",
    "with open(\"./promt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# JSON 파일의 내용을 기반으로 PromptTemplate 생성\n",
    "j_promt = PromptTemplate(\n",
    "    input_variables=config[\"input_variables\"],\n",
    "    template=config[\"template\"]\n",
    ")\n",
    "\n",
    "# 프롬프트 포맷 사용\n",
    "formatted_output = j_promt.format(country=\"호주\")\n",
    "print(formatted_output)\n",
    "CHAT.predict(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 에러 발생해서 아래와 같은 코드로 구성\n",
    "\n",
    "import yaml\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# YAML 파일을 UTF-8로 읽기\n",
    "with open(\"./promt.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# YAML 파일의 내용을 기반으로 PromptTemplate 생성\n",
    "y_promt = PromptTemplate(\n",
    "    input_variables=config[\"input_variables\"],\n",
    "    template=config[\"template\"]\n",
    ")\n",
    "\n",
    "# 프롬프트 포맷 사용\n",
    "formatted_output = y_promt.format(country=\"호주\")\n",
    "print(formatted_output)\n",
    "CHAT.predict(formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 프롬프트를 하나로 묶는 방법으로 PipelinePromptTemplate이 있음\n",
    "#탬플릿1\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "#탬플릿2\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "#탬플릿3\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "#3개의 탬플릿을 합치기 위한 틀\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "#prompts와 full_prompt을 통해 3개의 탬플릿을 묶음\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "# chain 제작\n",
    "chain = full_prompt | CHAT\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_llm_cache(InMemoryCache()) #캐시 메모리에 대화를 저장하는 코드.\n",
    "# set_debug(True) #디버그 모드 활성화. 로그를 더 상세하게 볼 수 있음\n",
    "\n",
    "# CHAT.predict(\"이탈리아 화덕 피자 만드는 방법\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 화덕 피자 만드는 방법\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [0ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 화덕 피자를 만드는 방법은 다음과 같습니다:\\n\\n1. 피자 반죽을 만듭니다: 밀가루, 물, 소금, 이스트를 섞어 부드러운 반죽을 만들어줍니다. 이를 냉장고에서 발효시켜줍니다.\\n\\n2. 토마토 소스를 만듭니다: 토마토 퓨레, 올리브 오일, 소금, 후추, 마늘을 섞어 소스를 만들어줍니다.\\n\\n3. 토핑을 준비합니다: 모차렐라 치즈, 페퍼로니, 버섯, 양파, 파프리카 등을 썰어 준비해둡니다.\\n\\n4. 화덕을 예열합니다: 이탈리아 화덕은 높은 열을 유지할 수 있는 화덕이기 때문에 높은 열을 유지할 수 있도록 예열해줍니다.\\n\\n5. 반죽을 펴서 피자를 만듭니다: 반죽을 펴서 동그랗게 모양을 만들고, 토마토 소스를 발라줍니다. 그리고 토핑을 얹어줍니다.\\n\\n6. 화덕에 넣고 굽습니다: 화덕 속에 피자를 넣고 약 1~2분간 굽습니다. 이때 피자를 자주 확인하여 너무 타지 않도록 주의해야 합니다.\\n\\n7. 피자를 꺼내고 마무리합니다: 피자를 꺼내서 올리브 오일이나 파슬리를 뿌려 마무리해줍니다.\\n\\n이렇게 하면 이탈리아 화덕 피자를 만들 수 있습니다. 맛있게 즐겨보세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"이탈리아 화덕 피자를 만드는 방법은 다음과 같습니다:\\n\\n1. 피자 반죽을 만듭니다: 밀가루, 물, 소금, 이스트를 섞어 부드러운 반죽을 만들어줍니다. 이를 냉장고에서 발효시켜줍니다.\\n\\n2. 토마토 소스를 만듭니다: 토마토 퓨레, 올리브 오일, 소금, 후추, 마늘을 섞어 소스를 만들어줍니다.\\n\\n3. 토핑을 준비합니다: 모차렐라 치즈, 페퍼로니, 버섯, 양파, 파프리카 등을 썰어 준비해둡니다.\\n\\n4. 화덕을 예열합니다: 이탈리아 화덕은 높은 열을 유지할 수 있는 화덕이기 때문에 높은 열을 유지할 수 있도록 예열해줍니다.\\n\\n5. 반죽을 펴서 피자를 만듭니다: 반죽을 펴서 동그랗게 모양을 만들고, 토마토 소스를 발라줍니다. 그리고 토핑을 얹어줍니다.\\n\\n6. 화덕에 넣고 굽습니다: 화덕 속에 피자를 넣고 약 1~2분간 굽습니다. 이때 피자를 자주 확인하여 너무 타지 않도록 주의해야 합니다.\\n\\n7. 피자를 꺼내고 마무리합니다: 피자를 꺼내서 올리브 오일이나 파슬리를 뿌려 마무리해줍니다.\\n\\n이렇게 하면 이탈리아 화덕 피자를 만들 수 있습니다. 맛있게 즐겨보세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 화덕 피자를 만드는 방법은 다음과 같습니다:\\n\\n1. 피자 반죽을 만듭니다: 밀가루, 물, 소금, 이스트를 섞어 부드러운 반죽을 만들어줍니다. 이를 냉장고에서 발효시켜줍니다.\\n\\n2. 토마토 소스를 만듭니다: 토마토 퓨레, 올리브 오일, 소금, 후추, 마늘을 섞어 소스를 만들어줍니다.\\n\\n3. 토핑을 준비합니다: 모차렐라 치즈, 페퍼로니, 버섯, 양파, 파프리카 등을 썰어 준비해둡니다.\\n\\n4. 화덕을 예열합니다: 이탈리아 화덕은 높은 열을 유지할 수 있는 화덕이기 때문에 높은 열을 유지할 수 있도록 예열해줍니다.\\n\\n5. 반죽을 펴서 피자를 만듭니다: 반죽을 펴서 동그랗게 모양을 만들고, 토마토 소스를 발라줍니다. 그리고 토핑을 얹어줍니다.\\n\\n6. 화덕에 넣고 굽습니다: 화덕 속에 피자를 넣고 약 1~2분간 굽습니다. 이때 피자를 자주 확인하여 너무 타지 않도록 주의해야 합니다.\\n\\n7. 피자를 꺼내고 마무리합니다: 피자를 꺼내서 올리브 오일이나 파슬리를 뿌려 마무리해줍니다.\\n\\n이렇게 하면 이탈리아 화덕 피자를 만들 수 있습니다. 맛있게 즐겨보세요!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAT.predict(\"이탈리아 화덕 피자 만드는 방법\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 와인을 만드는 방법\"\n",
      "  ]\n",
      "}\n",
      "와인을 만드는 과정은 다음과 같습니다:\n",
      "\n",
      "1. 수확: 와인을 만들기 위한 포도를 수확합니다. 수확 시기는 포도의 성숙도와 품종에 따라 다르며, 보통 가을에 이루어집니다.\n",
      "\n",
      "2. 탈건: 수확한 포도를 탈건하여 준비합니다. 이때 잘못된 포도나 잎, 줄기 등을 제거합니다.\n",
      "\n",
      "3. 짜기: 준비된 포도를 짜서 주스를 추출합니다. 이 과정을 발효라고도 합니다.\n",
      "\n",
      "4. 발효: 추출한 포도주스에 효모를 첨가하여 발효를 진행시킵니다. 발효 과정에서 설탕이 알코올로 변하며, 와인의 향과 맛이 형성됩니다.\n",
      "\n",
      "5. 압착: 발효가 완료된 와인을 압착하여 과즙을 분리합니다.\n",
      "\n",
      "6. 숙성: 압착한 와인을 숙성시켜 와인의 맛과 향이 더 풍부해지도록 합니다. 숙성은 다양한 방법으로 이루어질 수 있습니다.\n",
      "\n",
      "7. 필터링 및 병입: 와인을 필터링하여 불순물을 제거하고, 병에 담아 보관합니다.\n",
      "\n",
      "8. 보관: 와인은 적절한 온도와 습도에서 보관하여 최상의 맛과 향을 유지합니다.\n",
      "\n",
      "이렇게 완성된 와인은 즐기는 사람들에게 즐거움과 만족을 주는 음료수가 됩니다.\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [6.28s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"와인을 만드는 과정은 다음과 같습니다:\\n\\n1. 수확: 와인을 만들기 위한 포도를 수확합니다. 수확 시기는 포도의 성숙도와 품종에 따라 다르며, 보통 가을에 이루어집니다.\\n\\n2. 탈건: 수확한 포도를 탈건하여 준비합니다. 이때 잘못된 포도나 잎, 줄기 등을 제거합니다.\\n\\n3. 짜기: 준비된 포도를 짜서 주스를 추출합니다. 이 과정을 발효라고도 합니다.\\n\\n4. 발효: 추출한 포도주스에 효모를 첨가하여 발효를 진행시킵니다. 발효 과정에서 설탕이 알코올로 변하며, 와인의 향과 맛이 형성됩니다.\\n\\n5. 압착: 발효가 완료된 와인을 압착하여 과즙을 분리합니다.\\n\\n6. 숙성: 압착한 와인을 숙성시켜 와인의 맛과 향이 더 풍부해지도록 합니다. 숙성은 다양한 방법으로 이루어질 수 있습니다.\\n\\n7. 필터링 및 병입: 와인을 필터링하여 불순물을 제거하고, 병에 담아 보관합니다.\\n\\n8. 보관: 와인은 적절한 온도와 습도에서 보관하여 최상의 맛과 향을 유지합니다.\\n\\n이렇게 완성된 와인은 즐기는 사람들에게 즐거움과 만족을 주는 음료수가 됩니다.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"와인을 만드는 과정은 다음과 같습니다:\\n\\n1. 수확: 와인을 만들기 위한 포도를 수확합니다. 수확 시기는 포도의 성숙도와 품종에 따라 다르며, 보통 가을에 이루어집니다.\\n\\n2. 탈건: 수확한 포도를 탈건하여 준비합니다. 이때 잘못된 포도나 잎, 줄기 등을 제거합니다.\\n\\n3. 짜기: 준비된 포도를 짜서 주스를 추출합니다. 이 과정을 발효라고도 합니다.\\n\\n4. 발효: 추출한 포도주스에 효모를 첨가하여 발효를 진행시킵니다. 발효 과정에서 설탕이 알코올로 변하며, 와인의 향과 맛이 형성됩니다.\\n\\n5. 압착: 발효가 완료된 와인을 압착하여 과즙을 분리합니다.\\n\\n6. 숙성: 압착한 와인을 숙성시켜 와인의 맛과 향이 더 풍부해지도록 합니다. 숙성은 다양한 방법으로 이루어질 수 있습니다.\\n\\n7. 필터링 및 병입: 와인을 필터링하여 불순물을 제거하고, 병에 담아 보관합니다.\\n\\n8. 보관: 와인은 적절한 온도와 습도에서 보관하여 최상의 맛과 향을 유지합니다.\\n\\n이렇게 완성된 와인은 즐기는 사람들에게 즐거움과 만족을 주는 음료수가 됩니다.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as usage: #usage를 통해 모델에서 사용한 토큰 수 및 비용 등의 데이터를 볼 수 있음\n",
    "    CHAT.predict(\"와인을 만드는 방법\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='안녕!'), AIMessage(content='안뇽!')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리가 없으면 AI 모델은 이전의 대화를 기억하지 못한다. 그래서 우리는 memory를 사용해야한다.\n",
    "#return_messages=True는 저장한 내용을 챗모델에게 사용 할 경우 True 아니면 False\n",
    "\n",
    "#ConversationBufferMemory는 모든 대화 내용을 저장하는 방식\n",
    "CB_memory = ConversationBufferMemory(return_messages=True)\n",
    "# 모든 메모리는 save_context와 load_memory_variables를 갖고 있다. 각 메모리마다 작동 방식이 다를 뿐이다.\n",
    "CB_memory.save_context({\"input\":\"안녕!\"},{\"output\":\"안뇽!\"})\n",
    "\n",
    "CB_memory.load_memory_variables({})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5'),\n",
       "  HumanMessage(content='6'),\n",
       "  AIMessage(content='6'),\n",
       "  HumanMessage(content='7'),\n",
       "  AIMessage(content='7')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConversationBufferWindowMemory는 대화의 특정 부분만을 저장하는 방식\n",
    "CBW_memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input,output):\n",
    "    CBW_memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3) # k가 4라서 이거 이전의 메세지들은 저장되지 않음\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "add_message(6,6)\n",
    "add_message(7,7)\n",
    "\n",
    "CBW_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greets the AI in Korean and mentions that they are studying. The AI responds with enthusiasm."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'The human greets the AI in Korean and mentions that they are studying. The AI responds with enthusiasm.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConversationSummaryMemory는 대화를 요약하여 저장하는 방식 / 초기에는 저장 공간이 많이 요구되지만, 이후에는 저장공간을 많이 사용되지 않음.\n",
    "CS_memory = ConversationSummaryMemory(\n",
    "    llm = CHAT\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    CS_memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return CS_memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 유령개야. 오늘도 공부 중이야.\", \"와우! 완전 멋쪄\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greets the AI in Korean and mentions that they are studying today."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean and mentions that they are studying today.'),\n",
       "  AIMessage(content='와우! 완전 멋쪄'),\n",
       "  HumanMessage(content='지금은 파이썬 공부중이야.'),\n",
       "  AIMessage(content='와우! 파이썬 공부라니, 어떤걸 하고 있어?')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConversationSummaryMemory는 특정 갯수의 메세지까지는 그냥 저장하고, 이후에 새로운 메세지에 의해 밀려나는 기존 메시지는 요약되어 저장됨\n",
    "CSB_memory = ConversationSummaryBufferMemory(\n",
    "    llm=CHAT,\n",
    "    max_token_limit=100,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    CSB_memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return CSB_memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"안녕 나는 유령개야. 오늘도 공부 중이야.\", \"와우! 완전 멋쪄\")\n",
    "add_message(\"지금은 파이썬 공부중이야.\", \"와우! 파이썬 공부라니, 어떤걸 하고 있어?\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONENONE"
     ]
    }
   ],
   "source": [
    "#ConversationKGMemory은 대화를 요약하면서 동시에 핵심적 키워드를 추출하는 방식\n",
    "CKG_memory = ConversationKGMemory(\n",
    "    llm=CHAT,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    CKG_memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"안녕 나는 유령개야. 오늘도 공부 중이야.\", \"와우! 완전 멋쪄\")\n",
    "add_message(\"지금은 파이썬 공부중이야.\", \"와우! 파이썬 공부라니, 어떤걸 하고 있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONE"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CKG_memory.load_memory_variables({\"inputs\": \"지금 무엇을 공부하나요\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕, 유령개! 만나서 반가워요. 어떻게 도와드릴까요?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕, 유령개! 만나서 반가워요. 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=CHAT,\n",
    "    max_token_limit=80,\n",
    "    return_messages=True, # 이것도 있어야 작동하는듯?\n",
    "    memory_key=\"chat_history\" # 하단 탬플릿의 chat_history에 메모리에 저장된 내역을 전달하기 위한 설정값\n",
    ")\n",
    "\n",
    "# 메모리에 대화는 저장되나, 해당 로그가 기록이 llm에 전달되지 않고있음. 그래서, 아래와 같은 탬플릿 생성\n",
    "template = \"\"\"\n",
    "    당신은 인간과 대화할때 도움이 되는 인공지능입니다.\n",
    "\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=CHAT,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "#    verbose=True # 해당 값이 있으면 프롬프트가 실행될때 로그들을 확인 할 수 있음\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chain.predict(question=\"내 이름은 유령개야!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그럼 유령개라는 이름도 참 독특하고 멋있네요! 이름 추천 사이트에서 좋은 선택을 했네요. 어떤 도움이 필요하신가요?The human introduces themselves as \"Ghost Dog.\" The AI greets Ghost Dog and asks how it can help. Ghost Dog mentions that a naming site recommended their name."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그럼 유령개라는 이름도 참 독특하고 멋있네요! 이름 추천 사이트에서 좋은 선택을 했네요. 어떤 도움이 필요하신가요?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"작명 사이트에서 이걸 추천해줬어!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 유령개님의 이름을 알고 있습니다. 어떤 도움이 필요하신가요?The human introduces themselves as \"Ghost Dog\" and the AI greets them, complimenting their unique name suggested by a naming site. The AI asks how it can help Ghost Dog."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'네, 유령개님의 이름을 알고 있습니다. 어떤 도움이 필요하신가요?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"내 이름이 뭔지 알아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [SystemMessage(content='The human introduces themselves as \"Ghost Dog.\" The AI greets Ghost Dog and asks how it can help. The human mentions a naming website and the AI finds the name recommendation interesting and asks about the website.'),\n",
       "  HumanMessage(content='내 이름이 뭔지 알아?'),\n",
       "  AIMessage(content='당신의 이름은 \"Ghost Dog\"이라고 소개하셨죠. 저는 여러분의 대화 상대가 될 수 있습니다. 무엇을 도와드릴까요?')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 인간과 대화할때 도움이 되는 인공지능입니다.\n",
      "Human: 내 이름은 유령개야!\u001b[0m\n",
      "안녕, 유령개! 만나서 반가워. 무엇을 도와드릴까요?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕, 유령개! 만나서 반가워. 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=CHAT,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\" # 하단 탬플릿의 chat_history에 메모리에 저장된 내역을 전달하기 위한 설정값\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"당신은 인간과 대화할때 도움이 되는 인공지능입니다.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=CHAT,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True # 해당 값이 있으면 프롬프트가 실행될때 로그들을 확인 할 수 있음\n",
    ")\n",
    "chain.predict(question=\"내 이름은 유령개야!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 인간과 대화할때 도움이 되는 인공지능입니다.\n",
      "Human: 내 이름은 유령개야!\n",
      "AI: 안녕, 유령개! 만나서 반가워. 무엇을 도와드릴까요?\n",
      "Human: 너랑은 나눌 이야기 없어!\u001b[0m\n",
      "알겠어요, 만약 도움이 필요하거나 궁금한 점이 생기면 언제든지 물어주세요. 저는 여러분을 돕기 위해 여기 있어요.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The human introduces themselves as \"유령개.\" The AI responds with a greeting and asks how it can help."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'알겠어요, 만약 도움이 필요하거나 궁금한 점이 생기면 언제든지 물어주세요. 저는 여러분을 돕기 위해 여기 있어요.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"너랑은 나눌 이야기 없어!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 인간과 대화할때 도움이 되는 인공지능입니다.\n",
      "System: The human introduces themselves as \"유령개.\" The AI responds with a greeting and asks how it can help.\n",
      "Human: 너랑은 나눌 이야기 없어!\n",
      "AI: 알겠어요, 만약 도움이 필요하거나 궁금한 점이 생기면 언제든지 물어주세요. 저는 여러분을 돕기 위해 여기 있어요.\n",
      "Human: 난 게임을 좋아해. 특히 역사나 시뮬레이션. 아니면, 대전략류 게임을 좋아해\u001b[0m\n",
      "게임을 좋아하시는군요! 역사나 시뮬레이션, 대전략 게임을 즐기시는 것 같아요. 그런 게임들은 정말로 흥미로운 선택이죠. 어떤 게임을 좋아하시는지 더 자세히 알려주시면, 해당 장르의 다양한 게임을 추천해드릴 수 있어요. 혹시 좋아하는 게임이나 특정 장르의 게임이 있나요?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The human introduces themselves as \"유령개.\" The AI responds with a greeting and offers assistance. The human expresses their love for games, especially historical, simulation, or grand strategy games. The AI acknowledges their interest in those genres and offers to recommend games based on their preferences."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'게임을 좋아하시는군요! 역사나 시뮬레이션, 대전략 게임을 즐기시는 것 같아요. 그런 게임들은 정말로 흥미로운 선택이죠. 어떤 게임을 좋아하시는지 더 자세히 알려주시면, 해당 장르의 다양한 게임을 추천해드릴 수 있어요. 혹시 좋아하는 게임이나 특정 장르의 게임이 있나요?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"난 게임을 좋아해. 특히 역사나 시뮬레이션. 아니면, 대전략류 게임을 좋아해\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=CHAT,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"당신은 인간과 대화할때 도움이 되는 인공지능입니다.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | CHAT\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그렇군요! 게임을 좋아하는 건 멋진 취미네요. 역사나 시뮬레이션, 그리고 대전략류 게임을 즐기는 것 같아서 저도 흥미롭게 생각해요. 어떤 게임을 주로 즐기시나요? 함께 게임 이야기를 나눠보는 건 어떨까요?The human expresses their love for games, especially historical or simulation ones, and sometimes grand strategy games. The AI finds it fascinating and suggests sharing game stories together.content='그렇군요! 게임을 좋아하는 건 멋진 취미네요. 역사나 시뮬레이션, 그리고 대전략류 게임을 즐기는 것 같아서 저도 흥미롭게 생각해요. 어떤 게임을 주로 즐기시나요? 함께 게임 이야기를 나눠보는 건 어떨까요?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"난 게임을 좋아해. 특히 역사나 시뮬레이션. 아니면, 대전략류 게임을 좋아해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문명 6는 정말 인기 있는 게임이죠! 어떤 문명을 선택해서 플레이하는지 궁금하네요. 어떤 전략을 사용하거나 특별한 이야기가 있나요? 함께 게임 이야기를 공유해보는 건 어떨까요?The human expresses their love for games, especially historical or simulation ones, and sometimes grand strategy games. The AI finds it fascinating and suggests sharing game stories together. The human mentions playing Civilization 6 a lot recently.content='문명 6는 정말 인기 있는 게임이죠! 어떤 문명을 선택해서 플레이하는지 궁금하네요. 어떤 전략을 사용하거나 특별한 이야기가 있나요? 함께 게임 이야기를 공유해보는 건 어떨까요?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"최근에는 문명6 많이 한 거 같아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 당신이 최근에 Civilization 6를 자주 플레이한다고 말씀하셨죠! 역사적이고 시뮬레이션 요소가 있는 게임을 좋아하신다고 했던 것 같아요. 어떤 문명을 선택하고 어떤 전략을 사용하는지 궁금합니다! 함께 게임 이야기를 나누면 재미있을 것 같네요.The human expresses their love for games, especially historical or simulation ones, and sometimes grand strategy games. The AI finds it fascinating and suggests sharing game stories together. The human mentions playing Civilization 6 a lot recently. The AI remembers the human's preference for Civilization 6 and their interest in historical and simulation games. They suggest sharing game stories and are curious about the civilizations chosen and strategies used.content='네, 당신이 최근에 Civilization 6를 자주 플레이한다고 말씀하셨죠! 역사적이고 시뮬레이션 요소가 있는 게임을 좋아하신다고 했던 것 같아요. 어떤 문명을 선택하고 어떤 전략을 사용하는지 궁금합니다! 함께 게임 이야기를 나누면 재미있을 것 같네요.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내가 어떤 게임을 자주했다고 말했는지 기억해?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
