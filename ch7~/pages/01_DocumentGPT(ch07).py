from typing import Any
from uuid import UUID
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.output import ChatGenerationChunk, GenerationChunk
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

# ì›¹ í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

# ì±—íŒ…ì´ ìƒì„±ë˜ëŠ”ê±¸ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ í´ë˜ìŠ¤.
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)



# ë°ì´í„° ìºì‹±í•˜ê¸°. í•´ë‹¹ ì–´ë…¸í…Œì´ì…˜?ì´ ì¡´ì¬í•˜ë©´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì— ëŒ€í•´ì„  ì„ë² ë”©ì„ ë‹¤ì‹œ í•˜ì§„ ì•ŠìŒ.
@st.cache_data(show_spinner="íŒŒì¼ì„ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    # íŒŒì¼ ë‚´ìš© ì½ê¸°
    file_content = file.read()
    # íŒŒì¼ ì €ì¥ ê²½ë¡œ ìƒì„±
    file_path = f"./.cache/files/{file.name}"
    # ì´ì§„ íŒŒì¼ë¡œ ìƒì„± ë° ë‚´ìš© ì‘ì„±. ë‚´ìš©ì€ ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ... ì €ì¥í•´ì•¼ ì„ë² ë”© ê°€ëŠ¥
    with open(file_path, "wb") as f:
        f.write(file_content)
    # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì§€ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    # ë¬¸ì„œ ìª¼ê°œê¸° íˆ´
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    # ì €ì¥í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    loader = UnstructuredFileLoader(file_path)
    # íˆ´ë¡œ ë¬¸ì„œ ìª¼ê°œê¸°
    docs = loader.load_and_split(text_splitter=splitter)
    #ì„ë² ë”©í•˜ê³ , ìºì‹± ì €ì¥í•˜ê¸°
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    # ë°±í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±. ë°˜í™˜í•˜ì—¬ ì²´ì¸ì— ì‚¬ìš©
    retriever = vectorstore.as_retriever()
    return retriever

# ë‚´ìš© ì €ì¥
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

# ë©”ì„¸ì§€ ë³´ë‚´ê¸°. ë§Œì•½ì— ì‹ ê·œ ë©”ì„¸ì§€ë©´, sessionì— ì €ì¥í•¨.
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

# ê¸°ì¡´ ëŒ€í™” ë¡œê·¸ ì¶œë ¥í•´ì£¼ëŠ” ë¶€ë¶„ / ì´ë¯¸ ì €ì¥ëœ ëŒ€í™” ë¡œê·¸ì´ê¸°ì— ë•Œë¬¸ì— saveëŠ” falseë¡œ ì§€ì •
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )


# aií•œí…Œ ì œê³µí•  ë¬¸ì„œ ì œì‘ ë° ë°˜í™˜
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            ë‹¤ìŒ ë¬¸ë§¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ê·¸ëƒ¥ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ì•„ë¬´ê²ƒë„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ë‹¤ë§Œ "ê³ ë§ˆì›Œ"ì™€ ê°™ì´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì§€ ì•Šì•„ë„ í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì§§ê²Œ ëŒ€ë‹µí•´ë„ ë©ë‹ˆë‹¤.
            ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì•¼í•˜ë©°, ë˜ë„ë¡ ê²½ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)

### ë””ìì¸ ###
st.title("DocumentGPT")
st.markdown("""ì–´ì„œ ì˜¤ì„¸ìš”!!
            
ì±—ë´‡ì„ ì‚¬ìš©í•˜ì—¬, ë‹¹ì‹ ì´ ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!!!""")
### ë””ìì¸ ###

with st.sidebar:
    api=st.text_input("apií‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    if api:
        # ëª¨ë¸
        llm = ChatOpenAI(
            temperature=0.1,
            streaming=True,
            callbacks=[
                ChatCallbackHandler(),
            ],
            opne_api_key = api
        )
        file = st.file_uploader(
            "Upload a .txt .pdf or .docx file",
            type=["pdf", "txt", "docx"],
        )

# íŒŒì¼ì´ ë“¤ì–´ì˜¨ ê²½ìš°
if file:
    retriever = embed_file(file)
    send_message("ì¤€ë¹„ë¬ì–´ìš”! ì§ˆë¬¸í•´ì£¼ì„¸ìš”!", "ai", save=False)
    paint_history()
    message = st.chat_input("ë‹¹ì‹ ì´ ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì„ ì£¼ì„¸ìš”.")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)
        # send_message(response.content, "ai") ìƒë‹¨ì˜ st.chat_message("ai") ë•Œë¬¸ì— ì•Œì•„ì„œ aiê°€ ì“´ ê¸€ë¡œ íŒì •ë¨.
        # ë‹¤ë§Œ, ì €ì¥ì´ ì•ˆë˜ê¸°ì— ì €ì¥í•˜ê¸° ìœ„í•´ì„  llmì´ ì¢…ë£Œë˜ëŠ” ì‹œì ì— í•´ë‹¹ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•´ì¤˜ì•¼í•¨.
else:
    st.session_state["messages"] = []